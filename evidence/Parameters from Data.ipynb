{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Driven ABM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting to tie model parameters and outcomes to real world data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need, Unmet Need, and Base Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making use of ELSA data to derive base rates in the population for those in need of in home care, and those in need who do not receive it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext rmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from STATA format, recode variables for mobility, ADL, IADL to NA, True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cell magic `%%R` not found.\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "library(foreign)\n",
    "library(survey)\n",
    "library(plyr)\n",
    "elsa_4 <- read.dta(\"elsa/UKDA-5050-stata11_se/stata11_se/wave_4_elsa_data_v3.dta\")\n",
    "elsa_4$mobility_need <- as.logical(mapvalues(tolower(elsa_4$hemob96), c(\"mentioned\", \"not mentioned\", \"refusal\", \"don't know\", \"schedule not applicable\", \"item not applicable\"), c(F, T, NA, NA, NA, NA)))\n",
    "elsa_4$adl_need <- as.logical(mapvalues(tolower(elsa_4$headl96), c(\"mentioned\", \"not mentioned\", \"refusal\", \"don't know\", \"schedule not applicable\", \"item not applicable\"), c(F, T, NA, NA, NA, NA)))\n",
    "elsa_4$need <- elsa_4$mobility_need | elsa_4$adl_need #This isn't ideal, but there happen to be no NA values here.\n",
    "elsa_4$help <- as.logical(mapvalues(tolower(elsa_4$hehpa), c(\"yes\", \"no\", \"refusal\", \"don't know\", \"schedule not applicable\"), c(T, F, NA, NA, NA)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ELSA wave 4, loaded using the survey package. Specifically interested in the 65+ age group, so next we produce a subset of the full sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "elsa_4.svy <- svydesign(~idauniq, data=elsa_4, weights=~w4xwgt)\n",
    "elsa_4.svy.65plus <- subset(elsa_4.svy, indager > 64);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proportion of the population with a mobility, ADL or IADL need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "svymean(~need, design=elsa_4.svy.65plus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that the proportion of over 65s with some need for support is relatively high, at 70.5%.\n",
    "Of those in need, ELSA also asks if they receive any support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "svyby(~help, ~need, design=elsa_4.svy.65plus, FUN=svymean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which suggests that a slight majority of 52.8% receive no assistance.\n",
    "\n",
    "We can also look at some specific ADL needs: washing, and dressing. Coded as headlba, and headldr in the ELSA dataset. (Following http://www.palgrave-journals.com/pt/journal/v145/n1/pdf/pt201117a.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "svytable(~headldr+headlba, design=elsa_4.svy.65plus, Ntotal=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can see that ~ 74.8% don't need help with either, 6.5% need help only with washing, 9.7% need help only with dressing, and 9% require support with both.\n",
    "\n",
    "ELSA also asks whether people receive support with these tasks specifically (Variable = hehpw96\tVariable label = Whether receives help to wash/dress: no help received)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "svyby(~hehpw96, ~headlba+headldr, elsa_4.svy.65plus, svymean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that 38.6% of those who only need help washing receive support, 34% of those who need assistance to dress themselves, and 68.3% of those needing support for both.\n",
    "We can also look at an amalgamated version of this, where we combine both need groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "elsa_4$adlneed <- (tolower(elsa_4$headlba) == \"mentioned\") | (tolower(elsa_4$headldr) == \"mentioned\")\n",
    "elsa_4.svy <- svydesign(~idauniq, data=elsa_4, weights=~w4xwgt)\n",
    "elsa_4.svy.65plus <- subset(elsa_4.svy, indager > 64)\n",
    "print(svyby(~hehpw96, ~adlneed, design=elsa_4.svy.65plus, FUN=svymean))\n",
    "print(svytable(~adlneed, design=elsa_4.svy.65plus, Ntotal=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that a little over a quarter would fall into an amalgamated need group, with 47.5% of them receiving assistance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Psychologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also necessary to derive some basis for the prior beliefs of the agent populations, covering their expectations of receiving help, belief that they will be stigmatised for requesting it, and in the case of the assessors, how trusting they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resignation, and a lack of belief that support will be offered, or make a practical difference if it is, is a key obstacle to help seeking behaviour in the older population.\n",
    "\n",
    "Two sources are available to explore these beliefs - ZA4561: Eurobarometer 67.3: Health Care Service, Undeclared Work, EU Relations With Its Neighbor Countries, and Development Aid, May-June 2007, and the ONS OPN MCE surveys from 2009-11. Both ask if the respondent believes they will receive appropriate support if they require long term care."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eurobarometer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we are interested in the over 65 age group, and derive a subset accordingly.\n",
    "\n",
    "The question text is \"In the future do you think that you would be provided with the appropriate help and long-term care if you were to need it?\", coded as v184 in the dataset.\n",
    "\n",
    "Since Eurobarometer covers several countries, we also use the UK specific weights (v10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "eb <- read.dta(\"beliefs/Eurobarometer/ZA4561_v2-1-0.dta\")\n",
    "eb.svy <- svydesign(~v5, data=eb, weights=~v10)\n",
    "eb.svy.65plus <- subset(eb.svy, v584 > 64)\n",
    "svymean(~v184, eb.svy.65plus, na.rm=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that the majority of people (75.5%) do believe they will receive the right care, but only a minority are certain of this.\n",
    "\n",
    "In parameterisation terms, we can interpret this as equating to a distribution of pseudocounts, where ~26% of the population have a strong prior in favour, 49% a weak prior in favour, and so forth. Naturally, while this gives valence, and is suggestive of relative magnitude, it does not provide a measure of how much weight the prior is assigned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ONS OPN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OPN surveys are slightly more complex, as they cover a time period of approximately a year (we have collated the surveys into a single file, for convenience), and do not precisely correspond to the Eurobarometer question in that they are a binary choice with a \"Don't know\" option.\n",
    "\n",
    "Question text \"Can older people get the services they need to continue to live at home?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "ops <- read.csv(\"beliefs/OPS/MCE/mce.csv\", header=T)\n",
    "ops$MCE_1 <- mapvalues(ops$MCE_1, c(1, 2, 3, 8), c(\"Yes\", \"No\", \"DK\", NA))\n",
    "ops.svy <- svydesign(~casenumber, data=ops, weights=~indwgt)\n",
    "ops.svy.65plus <- subset(ops.svy, rage > 64)\n",
    "svymean(~MCE_1, ops.svy.65plus, na.rm=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests a slightly less encouraging picture, in that while a majority of respondents still believe they can get the right care, this is reduced to 59.3%. The differing response structure may account for some of this difference, as may the change in government, and recession that occurred between the survey periods.\n",
    "\n",
    "Looking at this as a parameter, we can approach it as suggesting probabilities of positively and negatively valenced psuedocounts, with 26% of the population having an uninformative prior. Magnitude is, again, not derivable from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stigma is difficult to quantify, and whether the inhibitory factor should even be referred to as such is uncertain - it might equally represent the disutility associated with loss of social identity, or the concern of being a burden.\n",
    "\n",
    "With this said, we can make some assessment of how far the over 65 population expects to be regarded by others. To this end, we make use of wave 4 of the ESS (2008), which contained a special section on ageing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "ess <- read.dta(\"payoffs/social\\ costs/ESS/ESS4GB.stata/ESS4GB.dta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wave 4 of the ESS contains several likert scale items relevant to this situation, which assess what the respondent belives the attitude of the majority of society is towards those in their 70s.\n",
    "We approach these on the basis that they are true likert scale items, and combine them to form a likert scale.\n",
    "Specifically, this are of the form \"Most people view those over 70..\"\n",
    "\n",
    "    As\n",
    "        Competent (v70comp)\n",
    "        Friendly (v70frnd)\n",
    "        Having high moral standards (v70mrst)\n",
    "    With\n",
    "        Pity (v70pity)\n",
    "        Envy (v70envy)\n",
    "        Contempt (v70cntm)\n",
    "        Respect (v70resp)\n",
    "        Admiration (v70adm)\n",
    "\n",
    "All coded as 0-4 from not at all likely, to very likely to be viewed as such. We exclude cases where the respondent refused to answer, didn't know, or gave no answer to any of the questions. Since pity, envy, and contempt are negative attitudes, we invert the scales for these measures. We can then derive a scale either by taking the average of scale items for each respondent, or by summation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "mapfrom <- c(\"Not at all likely to be viewed that way\", \"Very likely to be viewed that way\", \"Refusal\", \"Don't know\", \"No answer\")\n",
    "mapto <- c(0, 4, NA, NA, NA)\n",
    "ess$v70comp <- as.numeric(as.character(mapvalues(ess$v70comp, mapfrom, mapto)))\n",
    "ess$v70frnd <- as.numeric(as.character(mapvalues(ess$v70frnd, mapfrom, mapto)))\n",
    "ess$v70mrst <- as.numeric(as.character(mapvalues(ess$v70mrst, mapfrom, mapto)))\n",
    "ess$v70pity <- as.numeric(as.character(mapvalues(ess$v70pity, mapfrom, mapto)))\n",
    "ess$v70envy <- as.numeric(as.character(mapvalues(ess$v70envy, mapfrom, mapto)))\n",
    "ess$v70cntm <- as.numeric(as.character(mapvalues(ess$v70cntm, mapfrom, mapto)))\n",
    "ess$v70resp <- as.numeric(as.character(mapvalues(ess$v70resp, mapfrom, mapto)))\n",
    "ess$v70adm <- as.numeric(as.character(mapvalues(ess$v70adm, mapfrom, mapto)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before combining the items, we should check the validity of doing so by calculating Cronbach's alpha for the combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(ltm)\n",
    "df <- ess[,c(\"v70comp\", \"v70frnd\", \"v70mrst\", \"v70pity\", \"v70envy\", \"v70cntm\", \"v70resp\", \"v70adm\")]\n",
    "print(descript(df)$alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An &alpha; value of 0.6 is not ideal, and could be improved by discarding the negative measures (this is unsuprising, given that a high rating would seem to imply the opposite to a high rating on a positive dimension.)\n",
    "\n",
    "Alternatively, we could deal with them as in The association between ageism and subjective age of older people in Europe, and invert them. Or, we could retain them and use a latent trait approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "ess$v70pityi <- 4 - ess$v70pity\n",
    "ess$v70envyi <- 4 - ess$v70envy\n",
    "ess$v70cntmi <- 4 - ess$v70cntm\n",
    "ess$stigma_mean <- rowMeans(ess[,c(\"v70comp\", \"v70frnd\", \"v70mrst\", \"v70pityi\", \"v70envyi\", \"v70cntmi\", \"v70resp\", \"v70adm\")])\n",
    "ess$stigma_sum <- (apply(ess[,c(\"v70comp\", \"v70frnd\", \"v70mrst\", \"v70pityi\", \"v70envyi\", \"v70cntmi\", \"v70resp\", \"v70adm\")], 1, sum))\n",
    "\n",
    "ess.svy <- svydesign(~idno, data=ess, weights=~pspwght)\n",
    "ess.svy.65plus <- subset(ess.svy, agea > 64)\n",
    "svymean(~stigma_mean, design=ess.svy.65plus, na.rm=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of averaged attitudes, at 2.7/4 suggests a tendency to believe that the majority of the population is relatively positive towards older adults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "svymean(~stigma_sum, design=ess.svy.65plus, na.rm=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also explore the distribution of perceived ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "svyhist(~stigma_mean, design=ess.svy.65plus)\n",
    "lines(svysmooth(~stigma_mean, ess.svy.65plus))\n",
    "\n",
    "svyhist(~stigma_sum, design=ess.svy.65plus)\n",
    "lines(svysmooth(~stigma_sum, ess.svy.65plus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks reasonably close to a (skewed) normal distribution, which we can check further with a qq-plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "qqnorm(subset(ess, agea > 64)$stigma_mean);qqline(subset(ess, agea > 64)$stigma_mean, col = 2)\n",
    "qqnorm(subset(ess, agea > 64)$stigma_sum);qqline(subset(ess, agea > 64)$stigma_sum, col = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The QQ-plot clearly indicates that the data is not quite normal, but is relatively close.\n",
    "We can also test for non-normality using Shapiro-Wilks, and look at skew and kurtosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(moments)\n",
    "print(shapiro.test(subset(ess, agea > 64)$stigma_mean))\n",
    "print(skewness(subset(ess, agea > 64)$stigma_mean, na.rm=T))\n",
    "print(kurtosis(subset(ess, agea > 64)$stigma_mean, na.rm=T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low p-value for the SW test, and the >0 skew suggest that a normal distribution may not be an ideal model for this parameter (but could be worse).\n",
    "\n",
    "We can also try finding a better fitting distribution. (Possibly we should be treating this as censored data?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(fitdistrplus)\n",
    "x <- subset(ess, agea > 64)\n",
    "x <- x[!is.na(x$stigma_mean), \"stigma_mean\"]\n",
    "hist(x)\n",
    "hist(4-x)\n",
    "ga <- fitdist(x, \"gamma\")\n",
    "print(ga)\n",
    "plot(ga)\n",
    "\n",
    "l <- fitdist(x, \"lnorm\")\n",
    "print(l)\n",
    "plot(l)\n",
    "\n",
    "n <- fitdist(x, \"norm\")\n",
    "print(n)\n",
    "plot(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the gamma, and lognormal distributions appear to be reasonable fits, so we compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "f <- gofstat(list(ga, l, n))\n",
    "print(f)\n",
    "print(f$adtest)\n",
    "print(f$kstest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the candidates pass a goodness of fit test, but this is not unexpected - we wouldn't anticipate the empirical data actually following one of the distributions. By eyeball, and by the metrics, the gamma distribution is the marginally better fit. This is also convenient, in having a lower bound of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine the extent to which this set of questions is a good aggregate measure, using latent trait analysis, and the R ltm package's graded response model. (Following along the lines of http://www.r-bloggers.com/item-response-modeling-of-customer-satisfaction-the-graded-response-model/)\n",
    "In fact, we're not as such interested in the individual item measures, but primarily in the underlying attitude - as such, distilling them to a latent trait may be a better parameterisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(ltm)\n",
    "df <- ess[, c(\"v70comp\", \"v70frnd\", \"v70mrst\", \"v70pity\", \"v70envy\", \"v70cntm\", \"v70resp\", \"v70adm\", \"agea\",\"stigma_mean\")]\n",
    "f <- grm(df[,1:8])\n",
    "pattern <- factor.scores(f, resp.pattern=df[,1:8])\n",
    "df$trait <- pattern$score.dat$z1\n",
    "print(qplot(trait, data=df, geom=\"density\"))\n",
    "print(qplot(trait, stigma_mean, data=df))\n",
    "\n",
    "print(qplot(trait, data=subset(df, df$agea > 64), main=\"65+\", geom=\"density\"))\n",
    "print(qplot(trait, stigma_mean, data=subset(df, df$agea > 64), main=\"65+\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we would like to be able to fit a distribution to this latent trait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "x <- (subset(df, df$agea > 64)$trait+4)/8\n",
    "fit <- fitdist(x, \"beta\")\n",
    "print(fit)\n",
    "plot(fit)\n",
    "f <- gofstat(fit)\n",
    "print(f)\n",
    "print(f$adtest)\n",
    "print(f$kstest)\n",
    "\n",
    "fitn <- fitdist(x, \"norm\")\n",
    "fitg <- fitdist(x, \"logis\")\n",
    "f <- gofstat(list(fitn, fitg))\n",
    "print(f)\n",
    "print(f$adtest)\n",
    "print(f$kstest)\n",
    "plot(fitg)\n",
    "print(fitg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this basis, the best fit would be a logistic distribution. We can operationalise this, by treating the draw as for x:1 ratio for/against stigma, depending on sign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring trust is challenging at best, and how to measure the specific sort of interpersonal trust represented in the priors for signal-type mappings is non-obvious.\n",
    "Constrained, as we are to existing data sources, we can attempt to approximate this using measures of generalised social trust. Happily, the 2008 ESS also provides a measure for this. Once again, this consists of several likert scale type items, which we will combine to form a scale.\n",
    "\n",
    "Specifically -\n",
    "\n",
    "\"Using this card, generally speaking, would you say that most people can be trusted, or that you can't be too careful in dealing with people? Please tell me on a score of 0 to 10, where 0 means you can't be too careful and 10 means that most people can be trusted.\" (ppltrst)\n",
    "\n",
    "\"Using this card, do you think that most people would try to take advantage of you if they got the chance, or would they try to be fair?\" (pplfair)\n",
    "\n",
    "\"Would you say that most of the time people try to be helpful or that they are mostly looking out for themselves?\" (pplhlp)\n",
    "\n",
    "Here, we are interested in the whole population, and work with the wider dataset. As before, refusals, don't knows, and no answers are treated as missing data and excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "mapto <- c(0, 10, NA, NA, NA)\n",
    "ess$ppltrst <- as.numeric(as.character(mapvalues(ess$ppltrst, c(\"You can't be too careful\", \"Most people can be trusted\", \"Refusal\", \"Don't know\", \"No answer\"), mapto)))\n",
    "ess$pplfair <- as.numeric(as.character(mapvalues(ess$pplfair, c(\"Most people try to take advantage of me\", \"Most people try to be fair\", \"Refusal\", \"Don't know\", \"No answer\"), mapto)))\n",
    "ess$pplhlp <- as.numeric(as.character(mapvalues(ess$pplhlp, c(\"People mostly look out for themselves\", \"People mostly try to be helpful\", \"Refusal\", \"Don't know\", \"No answer\"), mapto)))\n",
    "print(descript(ess[,c(\"ppltrst\", \"pplfair\", \"pplhlp\")])$alpha)\n",
    "ess$trust <- rowMeans(ess[,c(\"ppltrst\", \"pplfair\", \"pplhlp\")])\n",
    "ess$trust_sum <- apply(ess[,c(\"ppltrst\", \"pplfair\", \"pplhlp\")], 1, sum)\n",
    "ess.svy <- svydesign(~idno, data=ess, weights=~pspwght)\n",
    "svymean(~trust, ess.svy, na.rm=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "svyhist(~trust, design=ess.svy)\n",
    "lines(svysmooth(~trust, design=ess.svy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much less like a normal distribution, with considerable skew.\n",
    "\n",
    "A reasonable approximation might be the Weibull distribution, which we can fit using the fitdistrplus package. (I've scaled the lowest values up, to avoid difficulties with optimisation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "print(qplot(ess$trust, geom=\"density\"))\n",
    "x <- ess[!is.na(ess$trust), \"trust\"]+1\n",
    "wei<-fitdist(x,\"weibull\")\n",
    "plot(wei)\n",
    "print(wei)\n",
    "ga <- fitdist(x, \"gamma\")\n",
    "plot(ga)\n",
    "print(ga)\n",
    "norm <- fitdist(x, \"norm\")\n",
    "plot(norm)\n",
    "print(norm)\n",
    "f <- gofstat(list(wei, ga, norm))\n",
    "print(f)\n",
    "print(f$adtest)\n",
    "print(f$kstest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've also compared the Weibull fit to two other candidates (gamma, and normal). As with the stigma values, the goodness of fit tests fail, it is the better option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with stigma, we can look at this from a latent trait perspective. (Note that the latent trait measured here is arguably mistrust, with high ratings yielding low scores.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "t <- ess[,c(\"pplfair\", \"pplhlp\", \"ppltrst\")]\n",
    "ft <- grm(t)\n",
    "pt <- factor.scores(ft, resp.pattern=t)\n",
    "score <- pt$score.dat$z1\n",
    "print(qplot(score, geom=\"density\"))\n",
    "print(qplot(rowMeans(t), score))\n",
    "qqnorm(score);qqline(score, col = 2)\n",
    "print(shapiro.test(score))\n",
    "print(skewness(score, na.rm=T))\n",
    "print(kurtosis(score, na.rm=T))\n",
    "fitb <- fitdist((score+4)/8, \"beta\")\n",
    "print(fitb)\n",
    "plot(fitb)\n",
    "fitg <- fitdist((score+4)/8, \"logis\")\n",
    "fitn <- fitdist((score+4)/8, \"norm\")\n",
    "fitc <- fitdist((score+4)/8, \"cauchy\")\n",
    "fitl <- fitdist((score+4)/8, \"lnorm\")\n",
    "f <- gofstat(list(fitb, fitn, fitg, fitc, fitl))\n",
    "print(f)\n",
    "print(f$adtest)\n",
    "print(f$kstest)\n",
    "\n",
    "print(\"unstandardized\")\n",
    "fitn <- fitdist(score, \"norm\")\n",
    "fitc <- fitdist(score, \"logis\")\n",
    "f <- gofstat(list(fitn, fitc))\n",
    "print(f)\n",
    "print(f$adtest)\n",
    "print(f$kstest)\n",
    "print(fitn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In keeping with the theme, none of the distributions is a perfect fit. However a normal distribution is the least bad.\n",
    "\n",
    "Another, more time consuming alternative might be to fit a kernel distribution and sample from that. But that's not important at this juncture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cost of Care"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining a reasonable value for the cost of care is, in a sense, relatively simple. Expenditure, and activity for PSS (Personal Social Services) are available for the majority of local authorities across the UK. We may then work with the aggregate - the average cost of providing a year of care to an individual.\n",
    "\n",
    "It should be noted that this is potentially misleading, given large variability in how much care is required and received by individuals, and the cost of delivery.\n",
    "\n",
    "With this caveat in mind, we examine HSCIC data for PSS from the year 2008.\n",
    "\n",
    "In the 08/09 period, gross national expenditure on home care for over 65s was £2,022,861,000. However in this instance we are interested in the cost of delivering home care to one older adult for a year, as funded by local authorities. We can look at the average across England, and the distribution of cost for 150 LAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "older <- read.csv(\"payoffs/referral\\ costs/HSCIC/2008olderadults.csv\", header=T)\n",
    "library(ggplot2)\n",
    "print(summary(older))\n",
    "print(IQR(older$Annual))\n",
    "print(sd(older$Annual))\n",
    "print(qplot(Annual, data=older, geom=\"density\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average cost is around £7,800, but the standard deviation, IQR, and density plot all show that there is considerable national variation. While it may be reasonable to use a point estimate for the parameter, we can also explore fitting a distribution to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "qqnorm(older$Annual);qqline(older$Annual, col = 2)\n",
    "print(shapiro.test(older$Annual))\n",
    "lnorm <- fitdist(older$Annual, \"lnorm\")\n",
    "plot(lnorm)\n",
    "norm <- fitdist(older$Annual, \"norm\")\n",
    "plot(norm)\n",
    "f <- gofstat(list(norm, lnorm))\n",
    "print(f)\n",
    "print(f$adtest)\n",
    "print(f$kstest)\n",
    "print(lnorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A log normal distribution would appear to be a reasonable approximation for the distribution of care costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derived Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Parameter|Description|Value|Notes|Source|\n",
    "|---------|-----------|-----|-----|------|\n",
    "|p<sub>s</sub>(n)|Probability of a signaller needing support|0.70516|Covering any need|ELSA Wave 4|\n",
    "|p<sub>s</sub>(n)|Probability of a signaller needing support|0.25174|Washing/dressing need only|ELSA Wave 4|\n",
    "|X<sub>c</sub>|Cost to provide care for one year|7881|Mean value|Personal Social Services: Expenditure and Unit Costs, England, 2008/9. HSCIC|\n",
    "|X<sub>c</sub>|Cost to provide care for one year|7811|Median value|Personal Social Services: Expenditure and Unit Costs, England, 2008/9. HSCIC|\n",
    "|&alpha;<sub>referral</sub>|Belief in referral|Multinomial(YES=0.26386, yes=0.49122, no=0.17073, NO=0.07419)|3:1, 2:1 etc. multiplied by some constant k, maybe include noise here for ref by truth, ref by lie.[^1]|Eurobarometer|\n",
    "|&alpha;<sub>referral</sub>||Multinomial(dk=0.26357, no=0.14288, yes=0.59355)||ONS OPN|\n",
    "|&alpha;<sub>nice</sub>|Psuedocount of positive interactions|gamma(shape=37.29055, rate=13.95280)|Gamma distribution, drawing for x:1 prior bias multiplied by some constant k. Could make k an individual characteristic, drawn from a probability distribution, but have no evidence to draw from there.|ESS|\n",
    "|&alpha;<sub>nice</sub>|\"| logistic(location=0.5025573,scale=0.0630946)| Logistic distribution, drawing for x:1 with sign indicating bias, based on latent trait.| ESS|\n",
    "|&alpha;<sub>s==t</sub>|Psuedocount of honest/dishonest signals|normal(mean=0.0007080957,sd=0.9116370106)|Drawing for x:1 signal==type with sign indicating bias, based on latent trait.|ESS|\n",
    "\n",
    "[^1]:Another option here would be to discard the split reasoning on referral and just have a referred yes-no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use this information to generate some parameter sets. We will begin by generating an initial sensitivity analysis dataset, varying parameters we cannot derive from the data. A minimal interesting set of parameters might be these 9:\n",
    "\n",
    "### Parameters to vary\n",
    "\n",
    "|Parameter|Description|Notes|Range|\n",
    "|---------|-----------|-----|-----|\n",
    "|X<sub>h,g</sub>|Payoff for good quality of life|Likely to need to be in the same range as cost to refer.|0-10000|\n",
    "|X<sub>h,b</sub>|Cost for poor quality of life|Needs to be between X<sub>s</sub> and X<sub>h,g</sub>|0-10000|\n",
    "|X<sub>s</sub>|Social cost of asking for help from a punishing responder|Let's just say 0-10000 for a first look.|0-10000|\n",
    "|w<sub>s</sub>|Weight of shared information for signallers|Bounded between 0 and 1, may as well say uniform.|0-1|\n",
    "|w<sub>r</sub>|Weight of shared information for responders|\"|\n",
    "|q<sub>s</sub>|Probability of signallers sharing information|\"|\n",
    "|q<sub>r</sub>|Probability of responders sharing information|\"|\n",
    "|k<sub>s</sub>|Weight of priors for signallers|No obvious rationale for this, so say 1-100|1-100|\n",
    "|k<sub>r</sub>|Weight of priors for responders|\"|\n",
    "\n",
    "If we employ a latin hypercube design, this implies a 9 dimensional sampling space, with 4 dimensions left as unit, 3 rescaled to 0-10,000; and 2 to 1-100.\n",
    "\n",
    "We will begin with a 400 point, maximin design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import disclosuregame\n",
    "import disclosuregame.Agents.initors\n",
    "\n",
    "\n",
    "def designpoint(x_hg, x_hb, x_s, w_s, w_r, q_s, q_r, k_s, k_r):\n",
    "    point = {'game_args':{'referral_cost':7881,\"baby_payoff\":x_hg, \"mid_baby_payoff\":x_hb,\n",
    "                           \"mid_mid\":x_s, \"mid_low\":0, \"low_mid\":0,\"low_low\":0,\n",
    "                           \"women_share_prob\":q_s, \"mw_share_prob\":q_r},\n",
    "                    'signaller_args':{\"signals\":[0, 1], \"share_weight\":w_s},\n",
    "                    'responder_args':{\"signals\":[0, 1], \"share_weight\":w_r},\n",
    "                    'mw_weights':[1., 0.], \n",
    "                    'women_weights':[.75, .25],\n",
    "                    'signaller_initor':disclosuregame.Agents.initors.ebreferral_logisticstigma,\n",
    "                    'responder_initor':disclosuregame.Agents.initors.normalresponder,\n",
    "                    'signaller_init_args':{\"prior_weight\":k_s},\n",
    "                    'responder_init_args':{\"prior_weight\":k_r},\n",
    "                    'tag':\"sa_1\"}\n",
    "    return point\n",
    "\n",
    "def designpoint_sa2(x_hg, x_hb, x_s, w_s, w_r, q_s, q_r, k_s, k_r):\n",
    "    point = {'game_args':{'referral_cost':7881,\"baby_payoff\":x_hg, \"mid_baby_payoff\":x_hb,\n",
    "                           \"mid_mid\":x_s, \"mid_low\":0, \"low_mid\":0,\"low_low\":0,\n",
    "                           \"women_share_prob\":q_s, \"mw_share_prob\":q_r},\n",
    "                    'signaller_args':{\"signals\":[0, 1], \"share_weight\":w_s},\n",
    "                    'responder_args':{\"signals\":[0, 1], \"share_weight\":w_r},\n",
    "                    'mw_weights':[1., 0.], \n",
    "                    'women_weights':[.75, .25],\n",
    "                    'signaller_initor':disclosuregame.Agents.initors.onsreferral_logisticstigma,\n",
    "                    'responder_initor':disclosuregame.Agents.initors.normalresponder,\n",
    "                    'signaller_init_args':{\"prior_weight\":k_s},\n",
    "                    'responder_init_args':{\"prior_weight\":k_r},\n",
    "                    'tag':\"sa_1\"}\n",
    "    return point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyDOE import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = lhs(9, samples=400, criterion=\"maximin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "designs = []\n",
    "chunk = []\n",
    "chunksize = 1\n",
    "for design in samples:\n",
    "    design[:3] *= 10000\n",
    "    design[-2:] *= 100\n",
    "    design = designpoint(*design.tolist())\n",
    "    chunk.append(design)\n",
    "    if chunksize is not None and len(chunk) >= chunksize:\n",
    "        designs.append(chunk)\n",
    "        chunk = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(designs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from disclosuregame.experiments import args_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#args_write(designs, \"../experiments/sa_1/args\", \"sa_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmds = []\n",
    "cmd = '/home/jg1g12/pypy/bin/pypy run.py -R 25 -s %s -r %s --pickled-arguments /home/jg1g12/risky-aging-model/experiments/sa_1/args/sa_1_%d.args -f %d_sa_1 -i 1000 -d /scratch/jg1g12/%s_%s -g CarryingInformationGame --procs 1 --abstract-measures'\n",
    "for s, r in [(\"SharingBayesianPayoffSignaller\", \"SharingBayesianPayoffResponder\"), (\"SharingLexicographicSignaller\", \"SharingLexicographicResponder\"), (\"SharingProspectSignaller\", \"SharingProspectResponder\"), (\"SharingSignaller\", \"SharingResponder\")]:\n",
    "    for i in range(len(designs)):\n",
    "        cmds.append(cmd % (s, r, i, i, s, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"../experiments/sa_1/commands.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(cmds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "for i in range(400):\n",
    "    with open(\"../experiments/sa_1/args/sa_1_%d.args\" % i, \"rb\") as f:\n",
    "        arg = cPickle.load(f)\n",
    "        arg[0]['signaller_initor'] = disclosuregame.Agents.initors.onsreferral_logisticstigma\n",
    "        with open(\"../experiments/sa_2/args/sa_2_%d.args\" % i, \"wb\") as f2:\n",
    "            cPickle.dump(arg, f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Sharing Off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import numpy as np\n",
    "\n",
    "samples = lhs(5, samples=400, criterion=\"maximin\")\n",
    "designs = []\n",
    "chunk = []\n",
    "chunksize = 1\n",
    "for design in samples:\n",
    "    design[:3] *= 10000\n",
    "    design[-2:] *= 100\n",
    "    design = np.insert(design, -2, [0., 0., 0., 0.])\n",
    "    design = designpoint(*design.tolist())\n",
    "    chunk.append(design)\n",
    "    if chunksize is not None and len(chunk) >= chunksize:\n",
    "        designs.append(chunk)\n",
    "        chunk = []\n",
    "\n",
    "args_write(designs, \"../experiments/sa_3/args\", \"sa_3\")\n",
    "        \n",
    "cmds = []\n",
    "cmd = '/home/jg1g12/pypy/bin/pypy run.py -R 25 -s %s -r %s --pickled-arguments /home/jg1g12/risky-aging-model/experiments/sa_3/args/sa_3_%d.args -f %d_sa_3 -i 1000 -d /scratch/jg1g12/%s_%s -g CarryingInformationGame --procs 1 --abstract-measures'\n",
    "for s, r in [(\"SharingBayesianPayoffSignaller\", \"SharingBayesianPayoffResponder\"), (\"SharingLexicographicSignaller\", \"SharingLexicographicResponder\"), (\"SharingProspectSignaller\", \"SharingProspectResponder\"), (\"SharingSignaller\", \"SharingResponder\")]:\n",
    "    for i in range(len(designs)):\n",
    "        cmds.append(cmd % (s, r, i, i, s, r))\n",
    "with open(\"../experiments/sa_3/commands.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(cmds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"../experiments/sa_3/args/sa_3_398.args\") as f:\n",
    "    print cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
